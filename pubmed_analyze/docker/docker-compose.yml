
# docker-compose up --build


services:
  # Service pour Django
  django:
    build:
      context: .
      dockerfile: Dockerfile.django  # Assurez-vous d'avoir un Dockerfile spécifique pour Django
    container_name: django
    ports:
      - "8000:8000"  # Accès à Django sur le port 8000
    environment:
      - DJANGO_SETTINGS_MODULE=pubmed_analyze.settings  # Modifiez le chemin vers vos paramètres Django
      - DATABASE_URL=DATABASE_URL  # Connexion à la base de données via les variables d'environnement
    env_file:
      - .env
    volumes:
      - ..:/app  # Monte le code source local dans le conteneur
    depends_on:
      - elasticsearch  # Django dépend d'Elasticsearch pour ce projet
      - prometheus
      - db  # Ajout de la dépendance à la base de données
      # - ollama 
      - vllm

  # Service pour Elasticsearch
  elasticsearch:
    build:
      context: .
      dockerfile: Dockerfile.elasticsearch
    container_name: elasticsearch
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms2g -Xmx2g"
    volumes:
      - ./_data/elasticsearch_data:/usr/share/elasticsearch/data

  # Service pour PostgreSQL
  db:
    image: postgres:16
    container_name: postgres
    environment:
      - POSTGRES_USER=${DATABASE_USER}
      - POSTGRES_PASSWORD=${DATABASE_PASSWORD}
      - POSTGRES_DB=${DATABASE_NAME}
    ports:
      - "5432:5432"
    volumes:
      - ./data/postgresql:/var/lib/postgresql/data
    networks:
      - default

  # ollama:
  #   image: ollama/ollama:latest  # Vérifiez que l'image existe ou téléchargez l'outil Docker Ollama
  #   container_name: ollama
  #   ports:
  #     - "11434:11434"  # Port par défaut pour le serveur Ollama
  #   volumes:
  #     - ../../ollama_models:/root/.ollama/models  # Montez les données locales pour persister les modèles
  #   restart: always
  #   environment:
  #     - OLLAMA_NUM_PARALLEL=1

    # Service pour vLLM


  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm
    runtime: nvidia
    environment:
      - HUGGING_FACE_HUB_TOKEN=your_huggingface_token
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
    volumes:
      - ../../vllm_models/mistralai-Mistral-7B-v0.1-pubmed-summarization-5000-finetuned:/root/.cache/huggingface  # Montez le dossier du modèle local
    ports:
      - "8001:8000"  # Expose l'API sur le port 8001
    command: >
      --model /root/.cache/huggingface  




  # Service pour Prometheus
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml  # Assurez-vous d'avoir un fichier de configuration Prometheus
    ports:
      - "9090:9090"  # Accès à Prometheus sur le port 9090

  # Service pour Grafana
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"  # Accès à Grafana sur le port 3000
    depends_on:
      - prometheus

networks:
  default:
    name: django_network